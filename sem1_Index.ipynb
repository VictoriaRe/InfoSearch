{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Семинар 1 Индекс\n",
    "\n",
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### работа с файлами и папками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "curr_dir = os.getcwd()\n",
    "filepath = os.path.join(curr_dir, 'test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### os.path  \n",
    "путь до файла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/victoriaregina/infosearch/1 Index/test.txt\n",
      "test.txt\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# возвращает полный путь до папки/файла по имени файла / папки\n",
    "print(os.path.abspath(filepath))\n",
    "\n",
    "\n",
    "# возвращает имя файла / папки по полному ти до него\n",
    "print(os.path.basename(filepath))\n",
    "\n",
    "\n",
    "# проверить существование директории - True / False\n",
    "print(os.path.exists(curr_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### os.listdir  \n",
    "возвращает список файлов в данной директории"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints', 'lec1_Index.pdf', 'sem1_Index.ipynb', 'test.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(curr_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При обходе файлов не забывайте исключать системные директории, такие как .DS_Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### os.walk\n",
    "root - начальная директория  \n",
    "dirs - список поддиректорий (папок)   \n",
    "files - список файлов в этих поддиректориях  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/victoriaregina/infosearch/1 Index/lec1_Index.pdf\n",
      "/Users/victoriaregina/infosearch/1 Index/sem1_Index.ipynb\n",
      "/Users/victoriaregina/infosearch/1 Index/test.txt\n",
      "/Users/victoriaregina/infosearch/1 Index/.ipynb_checkpoints/sem1_Index-checkpoint.ipynb\n"
     ]
    }
   ],
   "source": [
    "for root, dirs, files in os.walk(curr_dir):\n",
    "    for name in files:\n",
    "        print(os.path.join(root, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __os.walk__ возвращает генератор, это значит, что получить его элементы можно только проитерировавшись по нему  \n",
    "но его легко можно превратить в list и увидеть все его значения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('/Users/victoriaregina/infosearch/1 Index',\n",
       "  ['.ipynb_checkpoints'],\n",
       "  ['lec1_Index.pdf', 'sem1_Index.ipynb', 'test.txt']),\n",
       " ('/Users/victoriaregina/infosearch/1 Index/.ipynb_checkpoints',\n",
       "  [],\n",
       "  ['sem1_Index-checkpoint.ipynb'])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(os.walk(curr_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### чтение файла "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = 'test.txt'\n",
    "\n",
    "\n",
    "# одним массивом  \n",
    "with open(fpath, 'r') as f:  \n",
    "    text = f.read() \n",
    "\n",
    "    \n",
    "#по строкам, в конце каждой строки \\n  \n",
    "with open(fpath, 'r') as f:   \n",
    "    text = f.readlines() \n",
    "\n",
    "    \n",
    "#по строкам, без \\n   \n",
    "with open(fpath, 'r') as f:   \n",
    "    text = f.read().splitlines() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напоминание про enumerate:    \n",
    "> При итерации по списку вы можете помимо самого элемента получить его порядковый номер    \n",
    "``` for i, element in enumerate(your_list): ...  ```    \n",
    "Иногда для получения элемента делают так -  ``` your_list[i] ```, не надо так"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Индекс \n",
    "\n",
    "Сам по себе индекс - это просто формат хранения данных, он не может осуществлять поиск. Для этого необходимо добавить к нему определенную метрику. Это может быть что-то простое типа булева поиска, а может быть что-то более специфическое или кастомное под задачу.\n",
    "\n",
    "Давайте посмотрим, что полезного можно вытащить из самого индекса.    \n",
    "По сути, индекс - это информация о частоте встречаемости слова в каждом документе.   \n",
    "Из этого можно понять, например:\n",
    "1. какое слово является самым часто употребимым / редким\n",
    "2. какие слова встречаются всегда вместе - так можно парсить твиттер, fb, форумы и отлавливать новые устойчивые выражения в речи\n",
    "3. как эти документы кластеризуются по N тематикам согласно словам, которые в них упоминаются "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Задача__: \n",
    "\n",
    "**Data:** Коллекция субтитров сезонов Друзьей. Одна серия - один документ.\n",
    "\n",
    "**To do:** Постройте небольшой модуль поискового движка, который сможет осуществлять поиск по коллекции документов.\n",
    "На входе запрос и проиндексированная коллекция (в том виде, как посчитаете нужным), на выходе отсортированный по релевантности с запросом список документов коллекции. \n",
    "\n",
    "Релизуйте:\n",
    "    - функцию препроцессинга данных\n",
    "    - функцию индексирования данных\n",
    "    - функцию метрики релевантности \n",
    "    - собственно, функцию поиска\n",
    "\n",
    "[download_friends_corpus](https://yadi.sk/d/yVO1QV98CDibpw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напоминание про defaultdict: \n",
    "> В качестве multiple values словаря рекомендую использовать ``` collections.defaultdict ```                          \n",
    "> Так можно избежать конструкции ``` dict.setdefault(key, default=None) ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "import nltk\n",
    "import collections\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/victoriaregina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "russian_stopwords = stopwords.words(\"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "corpus = []\n",
    "ep_per_seasons = {}\n",
    "names_docs = []\n",
    "\n",
    "main_dir = '/Users/victoriaregina/Downloads/friends'\n",
    "folders = [f for f in os.listdir(main_dir) if 'Store' not in f]\n",
    "\n",
    "\n",
    "def preprocessing(text):\n",
    "    normal_forms = []\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    tokens = [morph.parse(token)[0] for token in tokenizer.tokenize(text)]\n",
    "    normal_forms = [token.normal_form for token in tokens if token not in russian_stopwords and token.normal_form != 'это']\n",
    "    preproc_text = ' '.join(normal_forms)\n",
    "            \n",
    "    return preproc_text\n",
    "                   \n",
    "    \n",
    "for folder in folders:\n",
    "    for file in os.listdir(os.path.join(main_dir, folder)):\n",
    "        filepath = os.path.join(main_dir, folder, file)\n",
    "        names_docs.append(file)\n",
    "        \n",
    "        with open(filepath, 'r') as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        preproc_text = preprocessing(text)\n",
    "        corpus.append(preproc_text)\n",
    "        \n",
    "        ep_per_seasons[file] = folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165\n",
      "165\n"
     ]
    }
   ],
   "source": [
    "print(len(names_docs))\n",
    "print(len(ep_per_seasons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vec = TfidfVectorizer()\n",
    "X = vec.fit_transform(corpus)\n",
    "\n",
    "df = pd.DataFrame(X.toarray(), columns=vec.get_feature_names(), index=names_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(165, 15048)"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    query_preproc = preprocessing(query)\n",
    "    vec_query = vec.transform([query_preproc])\n",
    "    \n",
    "    vec_query = vec.transform([query_preproc])\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        vector = row.as_matrix()\n",
    "\n",
    "        cos_sim = cosine_similarity(vector.reshape(1, -1), vec_query)\n",
    "        cos_sim = np.asscalar(cos_sim)\n",
    "\n",
    "        results[cos_sim] = index\n",
    "\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Введите запрос: привет росс\n",
      "Вот 10 самых подходящих серий: \n",
      "1: Friends - 5x20 - The One With The Ride Along.ru.txt\n",
      "2: Friends - 5x01 - The One After Ross Says Rachel.ru.txt\n",
      "3: Friends - 7x04 - The One With Rachel's Assistant.ru.txt\n",
      "4: Friends - 2x01 - The One With Ross's New Girlfriend.DVDRip.ru.txt\n",
      "5: Friends - 4x24-25 - The One With Ross's Wedding (2).ru.txt\n",
      "6: Friends - 6x21 - The One Where Ross Meets Elizabeth's Dad.ru.txt\n",
      "7: Friends - 5x06 - The One With The Yeti.ru.txt\n",
      "8: Friends - 4x23-24 - The One With Ross's Wedding (1).ru.txt\n",
      "9: Friends - 2x01 - The One With Ross's New Girlfriend.ru.txt\n",
      "10: Friends - 5x02 - The One With All The Kissing.ru.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:13: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "query = input('Введите запрос: ')\n",
    "\n",
    "results = search(query)\n",
    "\n",
    "print('Вот 10 самых подходящих серий: ')\n",
    "\n",
    "for i, key in enumerate(sorted(results, reverse=True)[:10]):\n",
    "    print(str(i+1) + ': ' + results[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задания (a) и (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Самое частотное слово: ты 56.81246170527751\n",
      "Самое редкое слово: элегантный 0.01602731028921031\n"
     ]
    }
   ],
   "source": [
    "dic = {}\n",
    "\n",
    "for c in vec.get_feature_names():\n",
    "    dic[sum(df[c])] = c\n",
    "    \n",
    "print('Самое частотное слово:', dic[max(dic.keys())], max(dic.keys()))\n",
    "print('Самое редкое слово:', dic[min(dic.keys())], min(dic.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Слова, которые есть во всех текстах: быть, весь, да, думать, если, ещё, знать, как, мой, мочь, мы, на, не, нет, но, ну, он, она, просто, сказать, так, такой, то, тот, ты, хотеть, что, этот\n"
     ]
    }
   ],
   "source": [
    "in_all_docs = []\n",
    "d = df.isin([0])\n",
    "\n",
    "for c in vec.get_feature_names():\n",
    "    if sum(d[c]) == 0:\n",
    "        in_all_docs.append(c)\n",
    "        \n",
    "print('Слова, которые есть во всех текстах:', ', '.join(in_all_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "def popular_season_counting(df, character, seasons):\n",
    "    \n",
    "    data = df[character]\n",
    "    row_names = data.index\n",
    "\n",
    "    s1 = []\n",
    "    s2 = []\n",
    "    s3 = []\n",
    "    s4 = []\n",
    "    s5 = []\n",
    "    s6 = []\n",
    "    s7 = []\n",
    "    \n",
    "    count = {}\n",
    "\n",
    "    for i, value in enumerate(data):\n",
    "        if seasons[row_names[i]] == 'Friends - season 1':\n",
    "            s1.append(value)\n",
    "        elif seasons[row_names[i]] == 'Friends - season 2':\n",
    "            s2.append(value)\n",
    "        elif seasons[row_names[i]] == 'Friends - season 3':\n",
    "            s3.append(value)\n",
    "        elif seasons[row_names[i]] == 'Friends - season 4':\n",
    "            s4.append(value)\n",
    "        elif seasons[row_names[i]] == 'Friends - season 5':\n",
    "            s5.append(value)\n",
    "        elif seasons[row_names[i]] == 'Friends - season 6':\n",
    "            s6.append(value)\n",
    "        else:\n",
    "            s7.append(value)\n",
    "    keys = [numpy.sum(s1), numpy.sum(s2), numpy.sum(s3), numpy.sum(s4), numpy.sum(s5),\n",
    "       numpy.sum(s6), numpy.sum(s7)]\n",
    "    values = ['Friends - season 1', 'Friends - season 2', 'Friends - season 3', 'Friends - season 4',\n",
    "             'Friends - season 5', 'Friends - season 6', 'Friends - season 7']\n",
    "    count = dict(zip(keys, values))\n",
    "    return count[max(count.keys())]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Самый популярный сезон у Моники: Friends - season 7\n"
     ]
    }
   ],
   "source": [
    "print('Самый популярный сезон у Моники:', popular_season_counting(df, 'моника', ep_per_seasons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Самый популярный сезон у Чендлера: Friends - season 6\n"
     ]
    }
   ],
   "source": [
    "print('Самый популярный сезон у Чендлера:', popular_season_counting(df, 'чендлер', ep_per_seasons))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Самое популярный герой: росс 5.320893775007355\n"
     ]
    }
   ],
   "source": [
    "d = {}\n",
    "characters = ['росс', 'фиби', 'моника', 'чендлер', 'джо', 'рэйчел']\n",
    "\n",
    "for character in characters:\n",
    "    try:\n",
    "        d[sum(df[character])] = character\n",
    "    except KeyError:\n",
    "        break\n",
    "\n",
    "print('Самое популярный герой:', d[max(d.keys())], max(d.keys()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
