{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Семинар 1 Индекс\n",
    "\n",
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### работа с файлами и папками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "curr_dir = os.getcwd()\n",
    "filepath = os.path.join(curr_dir, 'test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### os.path  \n",
    "путь до файла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/victoriaregina/infosearch/1 Index/test.txt\n",
      "test.txt\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# возвращает полный путь до папки/файла по имени файла / папки\n",
    "print(os.path.abspath(filepath))\n",
    "\n",
    "\n",
    "# возвращает имя файла / папки по полному ти до него\n",
    "print(os.path.basename(filepath))\n",
    "\n",
    "\n",
    "# проверить существование директории - True / False\n",
    "print(os.path.exists(curr_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### os.listdir  \n",
    "возвращает список файлов в данной директории"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints', 'lec1_Index.pdf', 'sem1_Index.ipynb', 'test.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(curr_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При обходе файлов не забывайте исключать системные директории, такие как .DS_Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### os.walk\n",
    "root - начальная директория  \n",
    "dirs - список поддиректорий (папок)   \n",
    "files - список файлов в этих поддиректориях  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/victoriaregina/infosearch/1 Index/lec1_Index.pdf\n",
      "/Users/victoriaregina/infosearch/1 Index/sem1_Index.ipynb\n",
      "/Users/victoriaregina/infosearch/1 Index/test.txt\n",
      "/Users/victoriaregina/infosearch/1 Index/.ipynb_checkpoints/sem1_Index-checkpoint.ipynb\n"
     ]
    }
   ],
   "source": [
    "for root, dirs, files in os.walk(curr_dir):\n",
    "    for name in files:\n",
    "        print(os.path.join(root, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __os.walk__ возвращает генератор, это значит, что получить его элементы можно только проитерировавшись по нему  \n",
    "но его легко можно превратить в list и увидеть все его значения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('/Users/victoriaregina/infosearch/1 Index',\n",
       "  ['.ipynb_checkpoints'],\n",
       "  ['lec1_Index.pdf', 'sem1_Index.ipynb', 'test.txt']),\n",
       " ('/Users/victoriaregina/infosearch/1 Index/.ipynb_checkpoints',\n",
       "  [],\n",
       "  ['sem1_Index-checkpoint.ipynb'])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(os.walk(curr_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### чтение файла "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = 'test.txt'\n",
    "\n",
    "\n",
    "# одним массивом  \n",
    "with open(fpath, 'r') as f:  \n",
    "    text = f.read() \n",
    "\n",
    "    \n",
    "#по строкам, в конце каждой строки \\n  \n",
    "with open(fpath, 'r') as f:   \n",
    "    text = f.readlines() \n",
    "\n",
    "    \n",
    "#по строкам, без \\n   \n",
    "with open(fpath, 'r') as f:   \n",
    "    text = f.read().splitlines() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напоминание про enumerate:    \n",
    "> При итерации по списку вы можете помимо самого элемента получить его порядковый номер    \n",
    "``` for i, element in enumerate(your_list): ...  ```    \n",
    "Иногда для получения элемента делают так -  ``` your_list[i] ```, не надо так"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Индекс \n",
    "\n",
    "Сам по себе индекс - это просто формат хранения данных, он не может осуществлять поиск. Для этого необходимо добавить к нему определенную метрику. Это может быть что-то простое типа булева поиска, а может быть что-то более специфическое или кастомное под задачу.\n",
    "\n",
    "Давайте посмотрим, что полезного можно вытащить из самого индекса.    \n",
    "По сути, индекс - это информация о частоте встречаемости слова в каждом документе.   \n",
    "Из этого можно понять, например:\n",
    "1. какое слово является самым часто употребимым / редким\n",
    "2. какие слова встречаются всегда вместе - так можно парсить твиттер, fb, форумы и отлавливать новые устойчивые выражения в речи\n",
    "3. как эти документы кластеризуются по N тематикам согласно словам, которые в них упоминаются "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Задача__: \n",
    "\n",
    "**Data:** Коллекция субтитров сезонов Друзьей. Одна серия - один документ.\n",
    "\n",
    "**To do:** Постройте небольшой модуль поискового движка, который сможет осуществлять поиск по коллекции документов.\n",
    "На входе запрос и проиндексированная коллекция (в том виде, как посчитаете нужным), на выходе отсортированный по релевантности с запросом список документов коллекции. \n",
    "\n",
    "Релизуйте:\n",
    "    - функцию препроцессинга данных\n",
    "    - функцию индексирования данных\n",
    "    - функцию метрики релевантности \n",
    "    - собственно, функцию поиска\n",
    "\n",
    "[download_friends_corpus](https://yadi.sk/d/yVO1QV98CDibpw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напоминание про defaultdict: \n",
    "> В качестве multiple values словаря рекомендую использовать ``` collections.defaultdict ```                          \n",
    "> Так можно избежать конструкции ``` dict.setdefault(key, default=None) ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import collections\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/victoriaregina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "russian_stopwords = stopwords.words(\"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nww = [] #array with names of series and words \n",
    "tokens = [] #all the tokens of all docs\n",
    "j = 0\n",
    "folders = [f for f in os.listdir('/Users/victoriaregina/Downloads/friends') if 'Store' not in f]\n",
    "for f in folders:\n",
    "    for file in os.listdir('/Users/victoriaregina/Downloads/friends/'+ f):\n",
    "        filepath = '/Users/victoriaregina/Downloads/friends/'+ f + '/'+ file\n",
    "        with open(filepath, 'r') as doc:\n",
    "            normal_forms = []\n",
    "            normal_forms.append(file)\n",
    "            doc = doc.read()\n",
    "            tokenizer = RegexpTokenizer(r'\\w+')\n",
    "            for t in tokenizer.tokenize(doc):\n",
    "                t = morph.parse(t)[0]\n",
    "                if t.normal_form not in russian_stopwords and t.normal_form != 'это' and not re.match(r'[0-9A-Za-z]', t.normal_form):\n",
    "                    normal_forms.append(t.normal_form)\n",
    "                    tokens.append(t.normal_form)\n",
    "            nww.append(normal_forms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = collections.Counter()\n",
    "for word in tokens:\n",
    "    count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "termdoc = dict.fromkeys(list(count), {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = []\n",
    "for file in nww:\n",
    "    episode = []\n",
    "    c = collections.Counter()\n",
    "    for word in file[1:]:\n",
    "        c[word] += 1\n",
    "    episode.append(file[0])\n",
    "    episode.append(dict(c))\n",
    "    word_freq.append(episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью обратного индекса посчитайте:  \n",
    "\n",
    "\n",
    "a) какое слово является самым частотным\n",
    "\n",
    "b) какое самым редким\n",
    "\n",
    "c) какой набор слов есть во всех документах коллекции\n",
    "\n",
    "d) какой сезон был самым популярным у Чендлера? у Моники?\n",
    "\n",
    "e) кто из главных героев статистически самый популярный? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 0\n",
    "k = 'k'\n",
    "top = {}\n",
    "in_all_docs = []\n",
    "for key, value in termdoc.items():\n",
    "    for i in range(len(word_freq)):\n",
    "        try:\n",
    "            termdoc[key].update(dict([(word_freq[i][0], word_freq[i][1][key])]))\n",
    "        except KeyError:\n",
    "            termdoc[key].update(dict([(word_freq[i][0], 0)]))\n",
    "    all_numb = []\n",
    "    for name, numb in value.items():\n",
    "        all_numb.append(numb)\n",
    "    top[key] = sum(all_numb)\n",
    "    if sum(all_numb)> m:\n",
    "        m = sum(all_numb)\n",
    "        k = key\n",
    "    if len(value) == 165 and 0 not in value.values():\n",
    "        in_all_docs.append(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Самое частотное слово: весь 3100\n"
     ]
    }
   ],
   "source": [
    "print(\"Самое частотное слово:\", k, max_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Самые редкие слова: придурь, горб, придача, старшеклассник, обмотать, чмо, зацикливаться, истерический, всхлип, лимож\n"
     ]
    }
   ],
   "source": [
    "unfreq = []\n",
    "for s in sorted(top.items(), key=lambda item: item[1])[:10]:\n",
    "    unfreq.append(s[0])\n",
    "print('Самые редкие слова:', ', '.join(unfreq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Набор слов, который есть во всех документах: весь, просто, мочь, знать, хотеть, сказать, ещё, думать\n"
     ]
    }
   ],
   "source": [
    "in_all_docs\n",
    "print('Набор слов, который есть во всех документах:', ', '.join(in_all_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Самый популярный герой: росс 1016\n"
     ]
    }
   ],
   "source": [
    "stat = {}\n",
    "top = []\n",
    "for i in ['росс', 'фиби', 'моника', 'чендлер', 'джо', 'рэйчел']:\n",
    "    n = top_sort[i]\n",
    "    stat[n] = i\n",
    "for v in stat.keys():\n",
    "    top.append(v)\n",
    "print('Самый популярный герой:', stat[max(top)], max(top))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
